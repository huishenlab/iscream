---
title: "Getting started with iscream"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Getting started with iscream}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
#bibliography: refs.bib
link-citations: yes
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Setup

### Loading *iscream*

The number of threads can be set before or after loading the library:

```{r}
options("iscream.threads" = 8)
library(iscream)
```

### Input BED files

Running this vignette requires downloading 2GB of single-cell whole genome
bisulfite sequencing (WGBS) BED files and tabix indices from this Zenodo
record: <https://zenodo.org/records/14733834>.

```{r download_data, eval = FALSE}
snmc_zip_path <- tempfile("snmcseq2")
snmc_dir <- tempdir()
download.file(
    "https://zenodo.org/records/14733834/files/sc_beds.zip",
    destfile = snmc_zip_path
)
unzip(snmc_zip_path, exdir = snmcseq2_dir)

genes_file <- tempfile("genes")
download.file(
    "https://zenodo.org/records/14733834/files/genes.bed",
    destfile = genes_file
)
```



100 human cell WGBS data from the snmc-seq2 dataset:

```{r}
bedfiles <- list.files(
  snmc_dir,
  pattern = "*.bed.gz$",
  full.names = TRUE
)[1:100]
```
### Regions

Since *iscream* is a region-based querying tool, we need to load some regions.
Here we'll be using 5000 gene body regions as the input:

```{r}
library(data.table)
regions <- fread(
  genes_file,
  col.names = c("chr", "start", "end", "gene")
)[1:5000]
head(regions)
```

## Running iscream

### Make tabix queries

The `tabix()` function can be used to query regions from BED files much like
the *tabix* shell command. It returns the queried lines from the BED files,
parsed into columns, as a `data.table`. `tabix()` is a generic BED files query
function. It has support for the BISCUIT, Bismark, and BSBolt aligners, set
with the `aligner` argument, to correctly set column names in the output.

If multiple input files are provided, they are queried in parallel. If `raw =
TRUE`, `tabix()` will return the same data as `Rsamtools::scanTabix()` does - a
named list of strings. For large queries, `tabix()` may currently seem
unresponsive, but a progress bar is in development to show its progress.

```{r}
system.time(tbx_query <- tabix(bedfiles, regions, col.names = c("beta", "coverage")))
tbx_query
```

### Get summary data

To get a summary of the information of the gene bodies use `summarize_regions`,
providing the gene name column as the feature column:

```{r}
system.time(summary_query <- summarize_regions(
  bedfiles,
  regions,
  columns = 4,
  col_names = "beta",
  feature_col = "gene")
)
head(summary_query)
```

Alternatively to use the methylation-specific `summarize_meth_regions()`
function:

```{r, eval = FALSE}
system.time(summary_query <- summarize_meth_regions(
  bedfiles,
  regions,
  feature_col = "gene")
)
```

### Build matrices

The `make_mat()` function queries and stores every locus within the input
regions across input files. Unlike `summarize_regions()` the output matrix
dimensions are unknown at runtime. Although usually quite fast, if the locus
count is very large and there are few overlaps in the loci between files, this
can take a long time. Here, gene bodies are large and the final matrix can
contain millions of loci. Further, with sparse data, chances are new loci are
found in every file.

Preallocating the number of rows, however, can drastically reduce runtime.
Since we got 45 million loci/CpGs from all the BED files with the tabix query
above, we can expect approximately between 5 and 10 million unique loci as
single-cell data has lower coverage than bulk. We already have the tabix query
so we can get the unique CpG count here and use it to preallocate the matrix,
reducing the number of matrix resizes. We'll add 100,000 extra on top of the
existing count to be safe since every avoided resize cuts off at least a couple
seconds from the runtime. Making tabix queries can be a relatively quick way to
approximate the CpG count of a dataset. If you haven't done a tabix query of
the full dataset, you can approximate how many CpGs to expect based on CpG
counts in one file and the coverage of your WGBS method. Here we make a matrix
of the beta-values in the 4th column:

```{r}
cpg.count <- tbx_query$start |> unique() |> length()
system.time(meth_mat <- make_mat(
  bedfiles,
  regions,
  column = 4,
  sparse = TRUE,
  prealloc = cpg.count + 1e5
))
str(meth_mat)
```

The output of `make_mat()` is a named list containing a matrix of beta values
and vectors of the sample names, chromosome names and positions of the loci.

If you want to make a BSseq object, you can use `make_bsseq_mat()` which makes
both beta/M-value and coverage value matrices. Its output can be used to
produce a BSseq object. However since BSseq cannot work with sparse matrices,
the two matrices would need to be converted to dense matrices first.

```{r, eval = FALSE}
meth_mat <- make_bsseq_mat(
  bedfiles,
  regions,
  sparse = TRUE,
  prealloc = cpg.count + 1e5
)
bs <- do.call(BSseq, meth_mat)
```

## Session info

```{r session}
sessionInfo()
```

<!-- vim: set filetype=rmd: -->
